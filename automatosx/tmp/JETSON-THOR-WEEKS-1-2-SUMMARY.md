# Jetson Thor Project: Weeks 1-2 Summary

**Project:** AkiDB 2.0 - NVIDIA Jetson Thor with ONNX Runtime + Qwen3 4B FP8
**Status:** Week 1 âœ… Complete, Week 2 Ready to Execute

---

## Overview

Strategic pivot from generic ARM edge devices to NVIDIA Jetson Thor platform (automotive/robotics) with Qwen3 4B embeddings for the $290B autonomous vehicle and industrial robotics market.

### Why Jetson Thor?

- **Market:** 6.4x larger than generic edge ($290B vs $45B by 2030)
- **Performance:** 2,000 TOPS (50x Mac ARM)
- **Competition:** No direct competitors (first mover advantage)
- **Use Cases:** Autonomous vehicles, industrial robotics, warehouse automation

### Why ONNX Runtime + TensorRT?

- **Performance:** 15-30ms P95 (vs 80-120ms with Candle)
- **Simplicity:** Single binary (vs 2-service TensorRT-LLM approach)
- **FP8 Support:** Native Blackwell GPU acceleration
- **Industry Standard:** ONNX is widely adopted

### Why Qwen3 4B FP8?

- **Quality:** 4096-dim embeddings (vs 384-dim MiniLM)
- **Multilingual:** 29 languages (critical for global automotive)
- **License:** Apache 2.0 (production-ready)
- **Size:** ~2GB FP8 (fits in Thor memory)

---

## Week 1: ONNX Foundation âœ… COMPLETE

**Goal:** Enhance ONNX provider with TensorRT Execution Provider support

### What Was Built

1. **Enhanced ONNX Provider** (`crates/akidb-embedding/src/onnx.rs`)
   - Added `ExecutionProviderConfig` enum (CoreML, TensorRT, CUDA, CPU)
   - Added `OnnxConfig` struct for flexible configuration
   - Implemented `with_config()` constructor
   - TensorRT EP with FP8 support, engine caching

2. **Multi-Platform Support**
   - Mac ARM: CoreML (existing)
   - Jetson Thor: TensorRT + FP8 (new)
   - Generic NVIDIA GPU: CUDA (new)
   - CPU fallback (existing)

3. **Testing**
   - 4 unit tests for configuration API (all passing)
   - Compiles successfully on Mac ARM

### Usage Example

```rust
use akidb_embedding::{OnnxConfig, ExecutionProviderConfig, OnnxEmbeddingProvider};
use std::path::PathBuf;

// Jetson Thor configuration
let config = OnnxConfig {
    model_path: PathBuf::from("models/qwen3-4b-fp8.onnx"),
    tokenizer_path: PathBuf::from("models/tokenizer.json"),
    model_name: "Qwen/Qwen2.5-4B".to_string(),
    dimension: 4096,
    max_length: 512,
    execution_provider: ExecutionProviderConfig::TensorRT {
        device_id: 0,
        fp8_enable: true,
        engine_cache_path: Some(PathBuf::from("/tmp/trt_cache")),
    },
};

let provider = OnnxEmbeddingProvider::with_config(config).await?;
```

### Week 1 Deliverables

- âœ… Enhanced ONNX provider (backward compatible)
- âœ… TensorRT Execution Provider support
- âœ… FP8 configuration
- âœ… 4 unit tests passing
- âœ… Documentation and examples
- âœ… Completion report

**Time:** ~2 hours (compressed from 5-day plan)

---

## Week 2: Model Integration & Validation ðŸ“‹ READY

**Goal:** Convert Qwen3 4B to ONNX, integrate with Rust, establish baseline performance

### 5-Day Plan

**Day 1 (Monday): Environment Setup**
- Install CUDA, TensorRT, Python dependencies
- Download Qwen3 4B model (~8GB)
- Clone AkiDB repository
- **Time:** 4 hours

**Day 2 (Tuesday): Model Conversion**
- Convert Qwen3 4B to ONNX FP8 using HuggingFace Optimum
- Validate ONNX model
- Test Python inference (TensorRT engine build: 2-5 min)
- Benchmark PyTorch vs ONNX
- **Time:** 5 hours

**Day 3 (Wednesday): Rust Integration**
- Create 12 integration tests
- Run tests (TensorRT engine cached after first run)
- Debug any failures
- Performance spot check
- **Time:** 5 hours

**Day 4 (Thursday): Performance Benchmarking**
- Create Criterion benchmark suite
- Run benchmarks (batch sizes 1-32)
- Manual performance tests
- Profile resource usage (GPU/CPU memory)
- **Time:** 5 hours

**Day 5 (Friday): Quality & Documentation**
- Validate embedding quality (>0.99 similarity vs HuggingFace)
- Test semantic similarity
- Create completion report
- Update project documentation
- **Time:** 5 hours

**Total:** ~24 hours over 5 days

### Expected Outcomes (Week 2)

**Performance (Baseline):**
- Latency (P95): 50-100ms @ batch 1
- Throughput: 10-20 QPS (single-threaded)
- GPU memory: 2-4GB

**Quality:**
- Cosine similarity: >0.99 vs HuggingFace
- Semantic similarity: PASS

**Testing:**
- 12 integration tests passing
- Criterion benchmarks complete
- Quality validation passed

### Week 2 Deliverables

- [ ] Qwen3 4B ONNX FP8 model
- [ ] 12 integration tests passing
- [ ] Baseline performance metrics
- [ ] Quality validation report
- [ ] Week 2 completion report

---

## Documentation Created

### Week 1 Documents

1. **JETSON-THOR-WEEK1-COMPLETION-REPORT.md** (~1,500 lines)
   - Executive summary
   - Implementation details
   - Code examples (Rust + scripts)
   - Testing results
   - Next steps

### Week 2 Documents

2. **JETSON-THOR-WEEK2-MODEL-INTEGRATION-PRD.md** (~2,000 lines)
   - 5-day implementation plan
   - Model conversion strategy
   - Integration testing approach
   - Performance validation methodology
   - Quality assurance procedures
   - Risk management
   - Code examples (Python + Rust)

3. **JETSON-THOR-WEEK2-ACTION-PLAN.md** (~1,800 lines)
   - Daily action items with checklists
   - Detailed bash commands for each task
   - Expected outputs and validation
   - Contingency plans
   - Daily status reports

### Strategic Documents (Previous)

4. **JETSON-THOR-STRATEGIC-PIVOT-ULTRATHINK.md**
   - Market analysis ($290B opportunity)
   - Technical comparison (ONNX vs Candle)
   - Decision framework

5. **TENSORRT-LLM-VS-ONNX-RUNTIME-ULTRATHINK.md**
   - Architecture decision analysis
   - Performance comparison (18-22ms vs 20-30ms)
   - Recommendation: ONNX Runtime (user intuition confirmed)

---

## Performance Targets

### Week 2 (Baseline - Current Goal)
- **Latency P95:** 50-100ms @ batch 1
- **Throughput:** 10-20 QPS (single-threaded)
- **Quality:** >0.99 cosine similarity
- **Status:** Acceptable for baseline

### Week 3 (Optimized - Future Goal)
- **Latency P95:** <30ms @ batch 1
- **Throughput:** >50 QPS (single-threaded)
- **Optimization:** TensorRT profiles, batch tuning, NVIDIA Nsight profiling

### Week 4+ (Production - End Goal)
- **Latency P95:** <20ms @ batch 1
- **Throughput:** >100 QPS (concurrent)
- **Features:** Multi-model support, K8s deployment, API integration

---

## Technical Stack

### Hardware
- **Platform:** NVIDIA Jetson Thor
- **GPU:** Blackwell architecture (2,000 TOPS)
- **Memory:** 64GB unified RAM
- **Storage:** 256GB NVMe SSD

### Software
- **OS:** Ubuntu 22.04 LTS (JetPack 6.0+)
- **CUDA:** 12.2+
- **TensorRT:** 9.0+
- **ONNX Runtime:** 1.17+ with TensorRT EP
- **Rust:** 1.75+
- **Python:** 3.10+ (for model conversion)

### Model
- **Model ID:** Qwen/Qwen2.5-4B
- **Parameters:** 4 billion
- **Dimension:** 4096
- **Precision:** FP8 (8-bit floating point)
- **Size:** ~2GB (vs 8GB FP32)
- **License:** Apache 2.0

---

## Key Files

### Source Code
```
crates/akidb-embedding/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ onnx.rs              # Enhanced ONNX provider (Week 1)
â”‚   â””â”€â”€ lib.rs               # Public exports
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ onnx_provider_test.rs        # Config tests (Week 1, 4 tests)
â”‚   â””â”€â”€ qwen3_integration_test.rs    # Integration tests (Week 2, 12 tests)
â””â”€â”€ benches/
    â””â”€â”€ qwen3_bench.rs       # Performance benchmarks (Week 2)
```

### Model Files (Week 2)
```
/opt/akidb/models/
â”œâ”€â”€ qwen3-4b-hf/             # HuggingFace PyTorch (Day 1)
â””â”€â”€ qwen3-4b-onnx-fp8/       # ONNX FP8 format (Day 2)
    â”œâ”€â”€ model.onnx           # ~2GB
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ config.json
```

### Documentation
```
automatosx/
â”œâ”€â”€ PRD/
â”‚   â”œâ”€â”€ JETSON-THOR-WEEK1-ONNX-FOUNDATION-PRD.md
â”‚   â””â”€â”€ JETSON-THOR-WEEK2-MODEL-INTEGRATION-PRD.md
â””â”€â”€ tmp/
    â”œâ”€â”€ JETSON-THOR-WEEK1-COMPLETION-REPORT.md
    â”œâ”€â”€ JETSON-THOR-WEEK2-ACTION-PLAN.md
    â”œâ”€â”€ JETSON-THOR-STRATEGIC-PIVOT-ULTRATHINK.md
    â”œâ”€â”€ TENSORRT-LLM-VS-ONNX-RUNTIME-ULTRATHINK.md
    â””â”€â”€ JETSON-THOR-WEEKS-1-2-SUMMARY.md (this file)
```

---

## Next Steps

### Immediate (Week 2 Execution)

**Monday (Day 1):**
1. Set up Jetson Thor development environment
2. Install CUDA, TensorRT, Python, Rust
3. Download Qwen3 4B model
4. Verify all dependencies

**Tuesday (Day 2):**
1. Convert Qwen3 4B to ONNX FP8
2. Validate ONNX model
3. Test Python inference
4. Benchmark vs PyTorch

**Wednesday (Day 3):**
1. Create 12 Rust integration tests
2. Run tests (wait for TensorRT engine build)
3. Debug any failures
4. Performance spot check

**Thursday (Day 4):**
1. Run Criterion benchmarks
2. Manual performance tests
3. Profile resource usage
4. Document baseline results

**Friday (Day 5):**
1. Quality validation (>0.99 similarity)
2. Create completion report
3. Update documentation
4. Archive logs and commit

### Future Weeks

**Week 3: Performance Optimization**
- Profile with NVIDIA Nsight Systems
- Tune TensorRT engine profiles
- Optimize batch sizes
- Target: <30ms P95 @ batch 1

**Week 4: Multi-Model Support**
- Add E5, BGE models
- Model registry
- Dynamic model loading

**Week 5: Production Deployment**
- Kubernetes Helm charts
- Docker containers
- CI/CD pipeline
- Blue-green deployment

**Week 6: API Integration**
- REST API with Qwen3
- gRPC API with Qwen3
- Load testing
- GA release v2.1.0

---

## Success Criteria

### Week 1 (COMPLETE âœ…)
- [x] Enhanced ONNX provider with TensorRT support
- [x] Multi-platform support (CoreML, TensorRT, CUDA, CPU)
- [x] FP8 configuration
- [x] 4 unit tests passing
- [x] Documentation complete

### Week 2 (READY ðŸ“‹)
- [ ] Qwen3 4B ONNX model converted
- [ ] 12 integration tests passing
- [ ] Baseline performance: 50-100ms P95
- [ ] Quality: >0.99 similarity
- [ ] Completion report

### Overall Project (Jetson Thor)
- [ ] Production-ready ONNX provider on Jetson Thor
- [ ] <30ms P95 latency @ 50+ QPS
- [ ] Multi-model support
- [ ] Kubernetes deployment
- [ ] GA release v2.1.0 with Jetson Thor support

---

## Resources

### Documentation
- **Week 1 PRD:** automatosx/PRD/JETSON-THOR-WEEK1-ONNX-FOUNDATION-PRD.md
- **Week 2 PRD:** automatosx/PRD/JETSON-THOR-WEEK2-MODEL-INTEGRATION-PRD.md
- **Week 2 Action Plan:** automatosx/tmp/JETSON-THOR-WEEK2-ACTION-PLAN.md
- **Strategic Analysis:** automatosx/tmp/JETSON-THOR-STRATEGIC-PIVOT-ULTRATHINK.md

### Code Examples
- See PRD documents for full Python and Rust code examples
- Model conversion scripts
- Integration test templates
- Benchmark templates

### External Links
- Qwen3: https://huggingface.co/Qwen/Qwen2.5-4B
- ONNX Runtime: https://onnxruntime.ai/
- TensorRT: https://developer.nvidia.com/tensorrt
- Jetson Thor: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-thor/

---

## Contact & Support

**Questions?**
- Check PRD documents for detailed guidance
- Review action plan for step-by-step instructions
- Refer to completion reports for lessons learned

**Issues?**
- TensorRT engine build failures â†’ Check TensorRT version (need 9.0+)
- Low quality â†’ Use FP16 instead of FP8
- Slow performance â†’ Accept baseline for Week 2, optimize in Week 3
- Test failures â†’ Debug individually, check logs

---

**Summary Version:** 1.0
**Last Updated:** 2025-11-11
**Status:** Week 1 Complete, Week 2 Ready to Execute
**Next Milestone:** Week 2 Day 1 - Environment Setup
