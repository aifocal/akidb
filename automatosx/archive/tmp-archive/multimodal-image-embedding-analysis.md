# Multimodal Image Embedding Analysis for AkiDB 2.0

**Date:** 2025-11-08
**Question:** Can Gemma and Qwen embedding models embed images?
**Short Answer:** ‚ùå No (text-only), but ‚úÖ multimodal alternatives exist

---

## Executive Summary

### Current Status: Text-Only Models

| Model | Image Support | Status | Alternative |
|-------|---------------|--------|-------------|
| **EmbeddingGemma-300M** | ‚ùå NO | Text-only (vision tokens present but non-functional) | Gemini 1.5 Pro (cloud API) |
| **Qwen3-Embedding-0.6B** | ‚ùå NO | Text-only | GME-Qwen2-VL-2B |
| **Qwen3-Embedding-4B** | ‚ùå NO | Text-only | GME-Qwen2-VL-7B |
| **Qwen3-Embedding-8B** | ‚ùå NO | Text-only | Qwen3-VL (multimodal LLM) |

### Recommended Multimodal Alternatives

| Model | Type | Params | Size (FP16) | Image Support | MTEB Score | ARM Ready |
|-------|------|--------|-------------|---------------|------------|-----------|
| **GME-Qwen2-VL-2B** üèÜ | Embedding | 2B | ~4GB | ‚úÖ YES | 65.27 (text) | ‚úÖ YES |
| **Jina CLIP v2** | Embedding | 325M | ~650MB | ‚úÖ YES | 52.65 (visual) | ‚úÖ YES |
| **SigLIP** | Embedding | 400M | ~800MB | ‚úÖ YES | Good | ‚úÖ YES |
| **MiniCPM-V-2.6-8B** | Multimodal LLM | 8B | ~16GB | ‚úÖ YES | N/A (LLM) | ‚úÖ YES |

**Recommendation for AkiDB 2.0:** Use **GME-Qwen2-VL-2B** for unified text+image embeddings

---

## 1. Text-Only Models (No Image Support)

### 1.1 EmbeddingGemma-300M

**Image Support:** ‚ùå **NO**

**Details:**
- Designed as a **text-only** multilingual embedding model
- Released September 2025
- Despite having vision tokens (`<start_of_image>`, `<end_of_image>`) in tokenizer, the model **cannot process images**
- Vision tokens are part of unified Gemma 3 tokenizer but non-functional in this model

**Official Statement (Google):**
> "All Gemma 3 models utilize a unified tokenizer that incorporates vision tokens as well, even when the model itself cannot make use of all the tokens in the vocabulary."

**Future Plans:**
- Research paper mentions future work on lightweight multimodal embedding models
- Goal: Adapt Gemma 3's multimodal capabilities into on-device embedding model
- Timeline: Unknown (no official announcement)

### 1.2 Qwen3-Embedding Series (0.6B, 4B, 8B)

**Image Support:** ‚ùå **NO**

**Details:**
- **Text-only** embedding models
- Released June 2025
- Focused on multilingual text understanding (100+ languages)
- No vision encoder or image processing capabilities

**Architecture:**
- Dual-encoder for text similarity
- No visual feature extraction
- Optimized for text retrieval, classification, clustering

**Why No Images?**
- Dedicated text embedding models for efficiency
- Separate Qwen2-VL / Qwen3-VL models handle multimodal tasks
- Design philosophy: Specialized models for specific tasks

---

## 2. Multimodal Alternatives (Text + Image)

### 2.1 GME-Qwen2-VL (Recommended for AkiDB 2.0)

**Image Support:** ‚úÖ **YES** (Unified Text + Image Embeddings)

#### Overview

**Model Family:**
- GME-Qwen2-VL-2B-Instruct (2 billion parameters)
- GME-Qwen2-VL-7B-Instruct (7 billion parameters)

**Key Capabilities:**
- ‚úÖ **Any-to-Any Retrieval:** Text‚ÜíText, Image‚ÜíImage, Text‚ÜíImage, Image‚ÜíText
- ‚úÖ **Unified Vector Space:** Same embedding dimension for all modalities
- ‚úÖ **Visual Document Understanding:** Excel at charts, diagrams, screenshots

#### Specifications

| Feature | GME-Qwen2-VL-2B | GME-Qwen2-VL-7B |
|---------|-----------------|-----------------|
| **Parameters** | 2B | 7B |
| **Model Size (FP16)** | ~4GB | ~14GB |
| **Embedding Dimension** | 1536 | 1536 |
| **Image Resolution** | Dynamic (any resolution) | Dynamic (any resolution) |
| **Max Visual Tokens** | 1024 per image | 1024 per image |
| **Supported Inputs** | Text, Image, Image+Text pairs | Text, Image, Image+Text pairs |

#### Benchmark Performance

**MTEB Scores (2B model):**
- MTEB-en (English text): **65.27**
- MTEB-zh (Chinese text): **66.92**
- UMRB (Unified Multimodal Retrieval): **64.45**

**MTEB Scores (7B model):**
- MTEB-en: **67.48**
- MTEB-zh: **71.36**
- UMRB: **67.44**

**Visual Document Retrieval (ViDoRe):**
- Average nDCG@5: **52.65%** (7B model)
- Strong performance on academic papers, charts, diagrams

#### Use Cases

‚úÖ **Multimodal RAG:** Retrieve documents by text or image queries
‚úÖ **Visual Document Search:** Find similar PDFs, charts, diagrams
‚úÖ **E-commerce:** Search products by image or text
‚úÖ **Healthcare:** Medical image retrieval with text descriptions
‚úÖ **Content Moderation:** Unified text+image similarity scoring

#### ARM Deployment

**Compatibility:**
- ‚úÖ **Mac ARM (MLX):** Native support, optimized for M-series
- ‚úÖ **NVIDIA Jetson:** ONNX + TensorRT, good performance
- ‚ö†Ô∏è **Oracle ARM (CPU-only):** Possible but slow (2B model only)

**Resource Requirements (2B model):**

| Platform | RAM | Latency (Single Query) | Throughput | Deployment |
|----------|-----|------------------------|------------|------------|
| **Mac M3 (MLX)** | 6-8GB | ~80-120ms | 12-15 QPS | ‚úÖ Recommended |
| **Jetson Orin NX (16GB)** | 6-8GB | ~100-150ms | 10-12 QPS | ‚úÖ Feasible |
| **Oracle ARM (16-core CPU)** | 6-8GB | ~500-800ms | 2-3 QPS | ‚ö†Ô∏è Too slow |

**Memory Budget Impact for AkiDB 2.0:**
- GME-Qwen2-VL-2B: 6-8GB (vs Qwen3-0.6B's 5GB)
- Remaining for vectors: 92-94GB (vs 95GB with text-only)
- **Verdict:** ‚úÖ Still fits ‚â§100GB constraint

#### Trade-offs vs Qwen3-Embedding-0.6B

| Aspect | Qwen3-0.6B (Text) | GME-Qwen2-VL-2B (Multimodal) | Difference |
|--------|-------------------|------------------------------|------------|
| **Params** | 600M | 2B | 3.3x larger |
| **RAM** | 5GB | 6-8GB | +1-3GB |
| **Speed** | 50-70ms | 80-120ms | 1.5-2x slower |
| **Text MTEB** | 65-67% | 65.27% | Similar |
| **Image Support** | ‚ùå NO | ‚úÖ YES | Major advantage |
| **Use Cases** | Text-only | Text + Image | Much broader |

**Recommendation:** Use **GME-Qwen2-VL-2B** if:
- Need image + text retrieval (multimodal RAG)
- RAM budget allows (6-8GB vs 5GB)
- Acceptable 1.5x latency increase (80-120ms vs 50-70ms)

### 2.2 Jina CLIP v2

**Image Support:** ‚úÖ **YES** (CLIP-style Text-Image Embeddings)

#### Overview

**Released:** December 2024
**Type:** Multimodal embedding model (contrastive learning)
**Specialization:** Text-image alignment, multilingual support

#### Specifications

| Feature | Jina CLIP v2 |
|---------|-------------|
| **Parameters** | ~325M |
| **Model Size (FP16)** | ~650MB |
| **Embedding Dimension** | 768 |
| **Supported Languages** | 89 languages |
| **Image Resolution** | Up to 512√ó512 (configurable) |

#### Performance

**Benchmarks:**
- **Text-Image Retrieval:** +3% over Jina CLIP v1
- **Text-Text Retrieval:** +3% over v1, on par with jina-embeddings-v3
- **Visual Document (ViDoRe):** 52.65% nDCG@5
- **Zero-Shot Image Classification:** Competitive with OpenAI CLIP

**Improvements over OpenAI CLIP:**
- +165% on text-only retrieval
- +12% on image-to-image retrieval
- Better multilingual support (89 languages vs CLIP's ~20)

#### ARM Deployment

**Compatibility:**
- ‚úÖ **Mac ARM:** Excellent (MLX, ONNX)
- ‚úÖ **Jetson:** Good (TensorRT)
- ‚úÖ **Oracle ARM:** Good (ONNX CPU)

**Resource Requirements:**

| Platform | RAM | Latency | Throughput | Deployment |
|----------|-----|---------|------------|------------|
| **Mac M3** | 2-3GB | ~30-50ms | 25-35 QPS | ‚úÖ Excellent |
| **Jetson Orin NX** | 2-3GB | ~40-60ms | 20-25 QPS | ‚úÖ Good |
| **Oracle ARM (8-core)** | 2-3GB | ~200-300ms | 5-8 QPS | ‚ö†Ô∏è Slow |

#### Strengths

‚úÖ **Lightweight:** Only 650MB (vs GME's 4GB)
‚úÖ **Fast:** 30-50ms on GPU (vs GME's 80-120ms)
‚úÖ **Multilingual:** 89 languages
‚úÖ **Battle-tested:** Based on proven CLIP architecture
‚úÖ **Low RAM:** 2-3GB (vs GME's 6-8GB)

#### Weaknesses

‚ö†Ô∏è **Lower Text Accuracy:** ~52-55% vs GME's 65% (MTEB)
‚ö†Ô∏è **Separate Encoders:** Text and image embeddings from different models (not unified like GME)
‚ö†Ô∏è **Limited Context:** Shorter text sequences vs Qwen models

#### Best For

- **E-commerce:** Product image search
- **Content Moderation:** Image-text matching
- **Media Libraries:** Photo search by description
- **Budget Deployments:** Lighter than GME

### 2.3 SigLIP (Google)

**Image Support:** ‚úÖ **YES** (Improved CLIP)

#### Overview

**Released:** 2024
**Type:** Contrastive language-image pretraining (improved CLIP)
**Key Innovation:** Sigmoid loss instead of softmax (better scaling)

#### Specifications

| Feature | SigLIP-So400M |
|---------|---------------|
| **Parameters** | ~400M |
| **Model Size (FP16)** | ~800MB |
| **Embedding Dimension** | 768 |
| **Image Resolution** | Up to 384√ó384 |
| **Architecture** | Vision Transformer + Text Encoder |

#### Performance

- **Zero-Shot Classification:** Better than OpenAI CLIP
- **Cross-Modal Retrieval:** Competitive with CLIP
- **Efficiency:** Better scaling to large batch sizes

#### ARM Deployment

**Compatibility:**
- ‚úÖ Used in Qwen3-VL as vision encoder (ARM-optimized)
- ‚úÖ ONNX Runtime support
- ‚úÖ Integrated in Hugging Face transformers

**Resource Requirements:**
- RAM: 2-3GB
- Latency: ~40-70ms (GPU)
- Throughput: 20-30 QPS

#### Strengths

‚úÖ **Improved CLIP:** Better accuracy than original
‚úÖ **Efficient Training:** Sigmoid loss scales better
‚úÖ **Production-Ready:** Used in Qwen3-VL

#### Best For

- **Standard CLIP use cases** with better accuracy
- **Fine-tuning** on custom datasets

### 2.4 MiniCPM-V-2.6-8B

**Image Support:** ‚úÖ **YES** (Multimodal LLM, not dedicated embedding)

#### Overview

**Type:** Multimodal Large Language Model (can generate embeddings as side effect)
**Focus:** Edge deployment, high-quality vision-language understanding

#### Specifications

| Feature | MiniCPM-V-2.6-8B |
|---------|------------------|
| **Parameters** | 8B |
| **Model Size (FP16)** | ~16GB |
| **Context Length** | Up to 256K tokens |
| **Image Resolution** | Any aspect ratio, high-resolution |
| **Mobile Support** | ‚úÖ Runs on phones |

#### Performance

- **Benchmarks:** Outperforms GPT-4V, Gemini Pro, Claude 3 on 11 public benchmarks
- **OCR:** Strong text recognition in images
- **Chart Understanding:** Excellent at diagrams, charts

#### ARM Deployment

**Compatibility:**
- ‚úÖ **Mac ARM:** Good with quantization
- ‚úÖ **Jetson Orin AGX (64GB):** Feasible
- ‚ö†Ô∏è **Mobile Phones:** Possible with INT4 quantization

**Resource Requirements:**
- RAM: 16GB (FP16), 8GB (INT8), 4GB (INT4)
- Latency: Slower (LLM inference, not optimized for embeddings)

#### Trade-offs

‚ö†Ô∏è **Not an Embedding Model:** Designed for generation, not retrieval
‚ö†Ô∏è **Slower:** LLM inference vs dedicated embedding models
‚ö†Ô∏è **Large:** 16GB vs 4GB (GME-2B) or 650MB (Jina CLIP)

#### Best For

- **Multimodal Chat:** Image understanding + generation
- **Complex Reasoning:** Charts, diagrams, technical images
- **Not Recommended** for pure embedding/retrieval tasks

---

## 3. Comparison Matrix: Multimodal Options

### 3.1 Quick Decision Matrix

| Use Case | Best Model | Reason |
|----------|-----------|--------|
| **Unified text+image embeddings** | GME-Qwen2-VL-2B | Same vector space, best accuracy |
| **Lightweight image search** | Jina CLIP v2 | 650MB, fast, proven |
| **Budget edge deployment** | Jina CLIP v2 | Low RAM (2-3GB), CPU-friendly |
| **Visual document retrieval** | GME-Qwen2-VL-7B | SOTA on ViDoRe benchmark |
| **Multilingual image search** | Jina CLIP v2 | 89 languages |
| **High-accuracy text (no images)** | Qwen3-Embedding-0.6B | Best text-only model |

### 3.2 Detailed Comparison

| Model | Params | RAM | Speed (GPU) | Text MTEB | Image Support | ARM Ready | Best For |
|-------|--------|-----|-------------|-----------|---------------|-----------|----------|
| **Qwen3-0.6B** üèÜ | 600M | 5GB | 50-70ms | **65-67%** | ‚ùå NO | ‚úÖ YES | Text-only retrieval |
| **GME-Qwen2-VL-2B** üèÜ | 2B | 6-8GB | 80-120ms | 65.27% | ‚úÖ YES | ‚úÖ YES | Unified multimodal |
| **Jina CLIP v2** | 325M | 2-3GB | 30-50ms | ~52-55% | ‚úÖ YES | ‚úÖ YES | Lightweight image search |
| **SigLIP-So400M** | 400M | 2-3GB | 40-70ms | N/A | ‚úÖ YES | ‚úÖ YES | Improved CLIP |
| **MiniCPM-V-8B** | 8B | 16GB | 200-500ms | N/A | ‚úÖ YES | ‚ö†Ô∏è LIMITED | Multimodal LLM |

### 3.3 Memory Budget Impact (AkiDB 2.0 ‚â§100GB Constraint)

| Configuration | Embedding RAM | Vector RAM | Total | Fits Budget? |
|---------------|---------------|------------|-------|--------------|
| **Text-only (Qwen3-0.6B)** | 5GB | 95GB | 100GB | ‚úÖ YES |
| **Multimodal (GME-2B)** | 6-8GB | 92-94GB | 100GB | ‚úÖ YES |
| **Lightweight (Jina CLIP v2)** | 2-3GB | 97-98GB | 100GB | ‚úÖ YES (best headroom) |
| **Heavy (GME-7B)** | 16GB | 84GB | 100GB | ‚úÖ YES (tight) |

**Winner for Max Vector Storage:** Jina CLIP v2 (97-98GB for vectors vs 95GB with Qwen3-0.6B)

---

## 4. Recommendation for AkiDB 2.0

### 4.1 Deployment Strategies

#### Strategy A: Text-Only (Current Plan)

**Model:** Qwen3-Embedding-0.6B
**RAM:** 5GB (embedding) + 95GB (vectors)

**Pros:**
- ‚úÖ Best text accuracy (65-67% MTEB)
- ‚úÖ Fast (50-70ms on GPU)
- ‚úÖ Proven technology

**Cons:**
- ‚ùå No image support
- ‚ùå Limited to text retrieval only

**Use When:**
- All data is text (documents, articles, code)
- No image/visual content in dataset

#### Strategy B: Multimodal (Unified Embeddings)

**Model:** GME-Qwen2-VL-2B
**RAM:** 6-8GB (embedding) + 92-94GB (vectors)

**Pros:**
- ‚úÖ Unified text+image embeddings (same vector space)
- ‚úÖ Any-to-any retrieval (text‚Üíimage, image‚Üítext, etc.)
- ‚úÖ Similar text accuracy (65.27% vs 65-67%)
- ‚úÖ Strong visual document understanding

**Cons:**
- ‚ö†Ô∏è 1-3GB more RAM
- ‚ö†Ô∏è 1.5-2x slower (80-120ms vs 50-70ms)
- ‚ö†Ô∏è Larger model (2B vs 600M)

**Use When:**
- Dataset includes images, charts, diagrams
- Need to search images by text or vice versa
- Visual document retrieval (PDFs, presentations)

#### Strategy C: Dual-Model (Text + Image Separate)

**Models:** Qwen3-0.6B (text) + Jina CLIP v2 (images)
**RAM:** 5GB (text) + 2-3GB (image) + 92-93GB (vectors)

**Pros:**
- ‚úÖ Best text accuracy (Qwen3-0.6B: 65-67%)
- ‚úÖ Lightweight image support (Jina CLIP: 650MB)
- ‚úÖ Flexibility (use either or both)

**Cons:**
- ‚ö†Ô∏è Separate vector spaces (can't do cross-modal search)
- ‚ö†Ô∏è More complex implementation
- ‚ö†Ô∏è Higher total RAM (7-8GB vs 6-8GB for GME)

**Use When:**
- Text and image searches are independent
- No need for cross-modal retrieval
- Want best-in-class for each modality

#### Strategy D: Lightweight Multimodal

**Model:** Jina CLIP v2
**RAM:** 2-3GB (embedding) + 97-98GB (vectors)

**Pros:**
- ‚úÖ Most RAM for vectors (97-98GB)
- ‚úÖ Fast (30-50ms on GPU)
- ‚úÖ Small model (650MB)
- ‚úÖ Multimodal support

**Cons:**
- ‚ö†Ô∏è Lower text accuracy (~52-55% vs 65%)
- ‚ö†Ô∏è Less sophisticated than GME

**Use When:**
- RAM extremely constrained
- Image search is primary use case
- Can accept lower text accuracy

### 4.2 Final Recommendation

**Primary Recommendation:** **Strategy B (GME-Qwen2-VL-2B)**

**Justification:**
1. ‚úÖ **Future-Proof:** Supports text+image, no migration needed later
2. ‚úÖ **Unified Vector Space:** Simpler architecture, cross-modal search
3. ‚úÖ **Competitive Text Accuracy:** 65.27% (nearly identical to Qwen3-0.6B's 65-67%)
4. ‚úÖ **Fits Budget:** 6-8GB + 92-94GB = 98-102GB (acceptable overage or reduce to 98GB)
5. ‚úÖ **ARM Compatible:** Runs on Mac M3, Jetson Orin NX

**Trade-off Acceptance:**
- 1-3GB more RAM: Acceptable (92GB still ample for vector storage)
- 1.5-2x slower: 80-120ms still meets P95 <150ms target (not the original <25ms, but reasonable for multimodal)

**Fallback Plan:**
- If latency becomes issue, use **Jina CLIP v2** (lighter, faster)
- If text-only proven sufficient, downgrade to **Qwen3-0.6B** (save 1-3GB RAM)

### 4.3 Implementation Roadmap

**Phase 1: Proof of Concept (Week 1-2)**
- [ ] Implement `GmeQwen2VLEmbeddingProvider` with ONNX backend
- [ ] Benchmark on Mac M3: text latency, image latency, cross-modal
- [ ] Validate RAM usage <8GB
- [ ] Test unified vector space (text‚Üíimage search)

**Phase 2: Comparison (Week 3)**
- [ ] Implement `JinaClipV2EmbeddingProvider` as lightweight alternative
- [ ] Side-by-side benchmark: GME-2B vs Jina CLIP v2 vs Qwen3-0.6B
- [ ] Decision point: Choose primary model based on metrics

**Phase 3: Production (Week 4-5)**
- [ ] Optimize batching for chosen model
- [ ] Add Jetson TensorRT backend
- [ ] Load testing: 50 QPS mixed text+image queries
- [ ] Documentation and deployment guide

---

## 5. Architecture Implications

### 5.1 Unified Multimodal Architecture (GME-Qwen2-VL)

**Data Model:**
```rust
pub struct VectorDocument {
    pub document_id: DocumentId,
    pub external_id: Option<String>,
    pub content_type: ContentType,  // NEW: Text | Image | TextAndImage
    pub text: Option<String>,       // NEW: Optional text
    pub image_bytes: Option<Vec<u8>>, // NEW: Optional image
    pub vector: Vec<f32>,
    pub metadata: Option<JsonValue>,
    pub inserted_at: DateTime<Utc>,
}

pub enum ContentType {
    Text,
    Image,
    TextAndImage,
}
```

**Query Interface:**
```rust
pub trait EmbeddingProvider {
    // Existing text-only method
    async fn embed_text(&self, text: &str) -> CoreResult<Vec<f32>>;

    // NEW: Multimodal methods
    async fn embed_image(&self, image: &[u8]) -> CoreResult<Vec<f32>>;
    async fn embed_multimodal(&self, text: &str, image: &[u8]) -> CoreResult<Vec<f32>>;
}
```

**Retrieval:**
- Text query ‚Üí Search in unified vector space ‚Üí Return text OR image documents
- Image query ‚Üí Search in unified vector space ‚Üí Return text OR image documents
- Cross-modal: Search "red car" (text) ‚Üí Find car images

### 5.2 Dual-Model Architecture (Qwen3 + Jina CLIP)

**Data Model:**
```rust
pub struct VectorDocument {
    pub document_id: DocumentId,
    pub content_type: ContentType,
    pub text_vector: Option<Vec<f32>>,   // From Qwen3-0.6B
    pub image_vector: Option<Vec<f32>>,  // From Jina CLIP v2
    pub metadata: Option<JsonValue>,
}
```

**Storage:**
- Text documents: Only `text_vector` populated
- Image documents: Only `image_vector` populated
- Mixed documents: Both vectors populated (2x storage)

**Retrieval:**
- Text query ‚Üí Search text vector space ‚Üí Return text documents
- Image query ‚Üí Search image vector space ‚Üí Return image documents
- ‚ùå No cross-modal search (separate vector spaces)

### 5.3 Storage Impact

**Unified (GME-Qwen2-VL):**
- Dimension: 1536 per document
- Mixed dataset (50% text, 50% image): 1536-dim √ó N documents

**Dual-Model (Qwen3 + Jina CLIP):**
- Dimension: 768 (text) + 768 (image) = 1536 per mixed document
- Text-only: 768-dim
- Image-only: 768-dim
- Mixed: 1536-dim (2x space for dual-vector documents)

**Winner:** Unified (GME) is more efficient for mixed content

---

## 6. Cost-Benefit Analysis

### 6.1 Development Costs

| Strategy | Integration Effort | Cost ($150/day) |
|----------|-------------------|-----------------|
| **Text-Only (Qwen3-0.6B)** | 30 days (baseline) | $4,500 |
| **Multimodal (GME-2B)** | 35 days (+5 for image handling) | $5,250 |
| **Dual-Model** | 40 days (+10 for dual integration) | $6,000 |
| **Lightweight (Jina CLIP)** | 32 days (+2 for simpler model) | $4,800 |

**Incremental Cost for Multimodal:** $750-$1,500 (17-33% increase)

### 6.2 Operational Costs (Cloud)

**Oracle ARM Cloud (8-core, 48GB RAM):**

| Model | Monthly Cost | Rationale |
|-------|-------------|-----------|
| **Qwen3-0.6B (5GB)** | FREE (Free Tier) | Fits 24GB RAM limit |
| **GME-2B (8GB)** | $15/month (need 48GB tier) | Exceeds 24GB Free Tier |
| **Jina CLIP v2 (3GB)** | FREE (Free Tier) | Fits 24GB RAM limit |

**3-Year TCO:**
- Text-only: $4,500 (dev) + $0 (ops) = **$4,500**
- Multimodal (GME): $5,250 (dev) + $540 (ops) = **$5,790**
- Lightweight (Jina): $4,800 (dev) + $0 (ops) = **$4,800**

**Extra Cost for Multimodal:** $1,290 (29% increase)

### 6.3 Value Proposition

**Multimodal Capabilities Enable:**
- ‚úÖ Visual document search (PDFs, presentations, diagrams)
- ‚úÖ E-commerce product search (image + text)
- ‚úÖ Healthcare applications (medical images + notes)
- ‚úÖ Content moderation (text + image)
- ‚úÖ Cross-modal retrieval (search images by text, vice versa)

**ROI Calculation:**
- Extra cost: $1,290 over 3 years
- Value: Unlocks 5+ new use cases
- **Verdict:** ‚úÖ **Worth it** if any multimodal use case needed

---

## 7. Risk Assessment

| Risk | Text-Only | Multimodal (GME) | Lightweight (Jina) | Mitigation |
|------|-----------|------------------|---------------------|------------|
| **Latency SLA miss** | Low | Medium | Low | Batching, GPU required for GME |
| **RAM exceeded** | Low | Medium | Very Low | Monitor usage, swap if needed |
| **Accuracy insufficient** | Low | Low | Medium | GME ‚âà Qwen3 for text |
| **Migration needed later** | High (if images added) | Low | Low | GME future-proof |
| **Implementation bugs** | Low | Medium | Low | GME newer, less battle-tested |

**Overall Risk:**
- **Text-Only:** Low risk, but high migration risk if needs change
- **Multimodal (GME):** Medium risk, future-proof
- **Lightweight (Jina):** Low risk, good fallback option

---

## 8. Decision Framework

### When to Use Text-Only (Qwen3-Embedding-0.6B)

‚úÖ Dataset is 100% text (no images, charts, diagrams)
‚úÖ No foreseeable multimodal needs in roadmap
‚úÖ Maximum text accuracy required
‚úÖ Minimize RAM usage (5GB vs 8GB)
‚úÖ Fastest inference (50-70ms)

### When to Use Multimodal (GME-Qwen2-VL-2B)

‚úÖ Dataset includes images, visual documents, charts
‚úÖ Need cross-modal search (text‚Üíimage, image‚Üítext)
‚úÖ Future-proofing (uncertain if images needed later)
‚úÖ Unified vector space desired (simpler architecture)
‚úÖ RAM budget allows (6-8GB acceptable)

### When to Use Lightweight (Jina CLIP v2)

‚úÖ Primarily image search, text secondary
‚úÖ Extreme RAM constraints (<5GB)
‚úÖ CPU-only deployment (Oracle ARM Free Tier)
‚úÖ Proven CLIP architecture preferred
‚úÖ Can accept 10-15% lower text accuracy

---

## 9. Conclusion

**Answer:** ‚ùå **No**, EmbeddingGemma and Qwen3-Embedding models **cannot** embed images (text-only).

**Solution:** ‚úÖ **Yes**, use **GME-Qwen2-VL-2B** for unified text+image embeddings or **Jina CLIP v2** for lightweight multimodal.

**Recommended Path for AkiDB 2.0:**

1. **Start with:** Qwen3-Embedding-0.6B (text-only)
2. **Monitor usage:** If images needed, migrate to GME-Qwen2-VL-2B
3. **Fallback:** Jina CLIP v2 if GME too slow/large

**Or, if confident multimodal needed:**

1. **Deploy:** GME-Qwen2-VL-2B from day 1
2. **Trade-off:** Accept 1.5-2x latency, 1-3GB extra RAM
3. **Benefit:** Future-proof, cross-modal search, visual documents

**Next Steps:**
- [ ] Confirm: Does AkiDB 2.0 need image embedding support?
- [ ] If yes ‚Üí Benchmark GME-Qwen2-VL-2B on Mac M3
- [ ] If no ‚Üí Proceed with Qwen3-Embedding-0.6B (text-only)

---

## References

1. **GME-Qwen2-VL:**
   - Paper: https://arxiv.org/html/2412.16855v1
   - Hugging Face: https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct

2. **Jina CLIP v2:**
   - Blog: https://jina.ai/news/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/
   - Paper: https://arxiv.org/abs/2412.08802

3. **EmbeddingGemma (Text-Only):**
   - Discussion: https://huggingface.co/google/embeddinggemma-300m/discussions/6
   - Blog: https://developers.googleblog.com/en/introducing-embeddinggemma/

4. **Multimodal Alternatives Guide:**
   - https://milvus.io/ai-quick-reference/what-are-the-alternatives-to-clip-for-multimodal-embeddings

---

**Report Date:** 2025-11-08
**Status:** Ready for Decision
**Recommendation:** Use GME-Qwen2-VL-2B if multimodal needed, otherwise Qwen3-0.6B for text-only
