# ONNX Runtime + CoreML EP Embedding Provider PRD

**Date**: November 10, 2025
**Status**: ğŸš§ **IN PROGRESS** - Replacing Candle with ONNX+CoreML
**Target**: Qwen3-Embedding-0.6B on Mac ARM with CoreML GPU/ANE acceleration

---

## Executive Summary

**Decision**: Use ONNX Runtime with CoreML Execution Provider for Mac ARM GPU acceleration

**Why**:
- âœ… Rust + Mac GPU requirement (via ort crate + CoreML EP)
- âœ… Bypasses Candle/Metal coverage issues (layer-norm not supported)
- âœ… CoreML EP dispatches operations to GPU/ANE for optimal performance
- âœ… Pure inference workload (no training needed)

**Target Model**: [Qwen3-Embedding-0.6B-ONNX](https://huggingface.co/Alibaba-NLP/new-impl/Qwen3-Embedding-0.6B-ONNX) from Hugging Face

**Risk**: ONNX export quality + CoreML EP operator support (manageable with community models)

---

## Architecture

### Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AkiDB Service Layer               â”‚
â”‚   (akidb-service)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OnnxEmbeddingProvider             â”‚
â”‚   (crates/akidb-embedding)          â”‚
â”‚   - CoreML EP configuration         â”‚
â”‚   - Tokenization (tokenizers crate) â”‚
â”‚   - Tensor I/O (ndarray)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ONNX Runtime (ort crate v2)       â”‚
â”‚   - CoreML Execution Provider       â”‚
â”‚   - Graph optimization Level 3      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apple CoreML Framework            â”‚
â”‚   - Metal GPU acceleration          â”‚
â”‚   - ANE (Neural Engine) dispatch    â”‚
â”‚   - MLProgram format                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependencies (Cargo.toml)

```toml
[dependencies]
# ONNX Runtime with CoreML EP
ort = { version = "2", optional = true, features = ["coreml"] }
ndarray = { version = "0.15", optional = true }
tokenizers = { version = "0.15.0", optional = true }
hf-hub = { version = "0.3.2", optional = true, features = ["tokio", "online"] }

[features]
default = ["onnx"]  # ONNX+CoreML enabled by default
onnx = ["ort", "ndarray", "tokenizers", "hf-hub"]
mlx = ["pyo3"]      # Fallback provider
```

### CoreML Execution Provider Configuration

```rust
use ort::{
    environment::Environment,
    session::SessionBuilder,
    GraphOptimizationLevel,
    execution_providers::CoreMLExecutionProviderOptions,
};

let coreml_options = CoreMLExecutionProviderOptions {
    // Use all compute units (GPU + ANE + CPU)
    ml_compute_units: Some("ALL".into()),

    // MLProgram format (newer, better performance)
    model_format: Some("MLProgram".into()),

    // Allow dynamic input shapes (for variable batch sizes)
    require_static_input_shapes: Some(false),

    // Disable subgraph optimization (more predictable behavior)
    enable_on_subgraphs: Some(false),

    ..Default::default()
};

let session = SessionBuilder::new(&env)?
    .with_execution_providers([coreml_options.into()])?
    .with_optimization_level(GraphOptimizationLevel::Level3)?
    .with_model_from_file("qwen3_embedding_0.6b.onnx")?;
```

---

## Implementation Plan

### Phase 1: Model Acquisition & Validation (4-6 hours)

**Goal**: Get Qwen3-Embedding-0.6B ONNX model and verify integrity

#### Task 1.1: Download ONNX Model
```bash
# From Hugging Face Hub
huggingface-cli download \
  Alibaba-NLP/new-impl/Qwen3-Embedding-0.6B-ONNX \
  --local-dir models/qwen3-embedding-0.6b

# Or use Python script
python scripts/download_qwen3_onnx.py
```

**Deliverables**:
- `models/qwen3-embedding-0.6b/model.onnx` (~600MB)
- `models/qwen3-embedding-0.6b/tokenizer.json`
- Checksum verification

#### Task 1.2: Validate ONNX Model
```python
import onnx

model = onnx.load("models/qwen3-embedding-0.6b/model.onnx")
onnx.checker.check_model(model)

# Print model inputs/outputs
for input in model.graph.input:
    print(f"Input: {input.name}, shape: {input.type.tensor_type.shape}")
for output in model.graph.output:
    print(f"Output: {output.name}, shape: {output.type.tensor_type.shape}")
```

**Expected**:
- Input: `input_ids` (batch_size, seq_len), type: INT64
- Input: `attention_mask` (batch_size, seq_len), type: INT64
- Output: `last_hidden_state` (batch_size, seq_len, 768), type: FLOAT32

#### Task 1.3: Test with onnxruntime Python
```python
import onnxruntime as ort
import numpy as np

# Create CoreML session
sess_options = ort.SessionOptions()
sess = ort.InferenceSession(
    "models/qwen3-embedding-0.6b/model.onnx",
    providers=['CoreMLExecutionProvider', 'CPUExecutionProvider']
)

# Test inference
input_ids = np.array([[101, 2023, 2003, 102]], dtype=np.int64)
attention_mask = np.array([[1, 1, 1, 1]], dtype=np.int64)

outputs = sess.run(None, {
    'input_ids': input_ids,
    'attention_mask': attention_mask
})

print(f"Output shape: {outputs[0].shape}")
print(f"Output dtype: {outputs[0].dtype}")
```

**Success Criteria**:
- âœ… ONNX model loads without errors
- âœ… CoreML EP activates (check `sess.get_providers()`)
- âœ… Inference runs successfully
- âœ… Output dimension correct (768 for Qwen3-0.6B)

---

### Phase 2: Rust ONNX Provider Implementation (8-12 hours)

**Goal**: Implement OnnxEmbeddingProvider with CoreML EP in Rust

#### Task 2.1: Update onnx.rs with CoreML EP

**File**: `crates/akidb-embedding/src/onnx.rs`

```rust
use ort::{
    environment::Environment,
    session::SessionBuilder,
    GraphOptimizationLevel,
    execution_providers::CoreMLExecutionProviderOptions,
};

pub struct OnnxEmbeddingProvider {
    session: Arc<Session>,
    tokenizer: Arc<Tokenizer>,
    model_name: String,
    dimension: u32,
}

impl OnnxEmbeddingProvider {
    pub async fn new(model_path: &str, model_name: &str) -> EmbeddingResult<Self> {
        eprintln!("ğŸ”§ Initializing ONNX Runtime with CoreML EP...");

        // 1. Create environment
        let env = Environment::builder()
            .with_name("akidb-onnx")
            .build()
            .map_err(|e| EmbeddingError::Internal(format!("Environment: {}", e)))?;

        // 2. Configure CoreML EP
        let coreml_options = CoreMLExecutionProviderOptions {
            ml_compute_units: Some("ALL".into()),  // GPU + ANE + CPU
            model_format: Some("MLProgram".into()), // Newer format
            require_static_input_shapes: Some(false), // Dynamic batching
            enable_on_subgraphs: Some(false),      // Disable for stability
            ..Default::default()
        };

        // 3. Create session with CoreML EP
        let session = SessionBuilder::new(&env)?
            .with_execution_providers([coreml_options.into()])?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .with_intra_threads(4)?
            .with_model_from_file(model_path)?;

        eprintln!("âœ… CoreML EP enabled: {:?}", session.get_execution_providers());

        // 4. Load tokenizer from HF Hub
        let tokenizer = Self::load_tokenizer(model_name).await?;

        // 5. Detect embedding dimension from ONNX metadata
        let dimension = Self::get_model_dimension(&session)?;

        eprintln!("âœ… OnnxEmbeddingProvider initialized");
        eprintln!("   Model: {}", model_name);
        eprintln!("   Dimension: {}", dimension);
        eprintln!("   Execution Providers: CoreML, CPU");

        Ok(Self {
            session: Arc::new(session),
            tokenizer: Arc::new(tokenizer),
            model_name: model_name.to_string(),
            dimension,
        })
    }

    // ... rest of implementation (tokenization, inference, pooling)
}
```

#### Task 2.2: Implement Inference Pipeline

```rust
pub async fn embed_batch_internal(
    &self,
    texts: Vec<String>,
) -> EmbeddingResult<Vec<Vec<f32>>> {
    // 1. Tokenization
    let encodings = self.tokenize_batch(&texts)?;

    // 2. Prepare tensors (input_ids, attention_mask)
    let (input_ids, attention_mask) = self.prepare_tensors(&encodings)?;

    // 3. ONNX inference with CoreML EP
    let outputs = self.session.run(vec![
        ort::Value::from_array(input_ids.view())?,
        ort::Value::from_array(attention_mask.view())?,
    ])?;

    // 4. Extract last_hidden_state
    let last_hidden_state = outputs[0].try_extract_tensor::<f32>()?;

    // 5. Mean pooling with attention mask
    let pooled = self.mean_pooling(last_hidden_state, &attention_mask)?;

    // 6. L2 normalization
    let normalized = self.l2_normalize(pooled)?;

    Ok(normalized)
}
```

#### Task 2.3: Error Handling for CoreML EP

**Common Issues**:
- Unsupported operators â†’ Fallback to CPU
- Dynamic shapes not supported â†’ Use static length
- CoreML compilation failed â†’ Log warning, use CPU EP

```rust
fn create_session_with_fallback(
    env: &Environment,
    model_path: &str,
) -> EmbeddingResult<Session> {
    // Try CoreML first
    match SessionBuilder::new(env)?
        .with_execution_providers([CoreMLExecutionProviderOptions::default().into()])?
        .with_model_from_file(model_path)
    {
        Ok(session) => {
            eprintln!("âœ… Using CoreML EP");
            Ok(session)
        }
        Err(e) => {
            eprintln!("âš ï¸  CoreML EP failed: {}, falling back to CPU", e);
            SessionBuilder::new(env)?
                .with_model_from_file(model_path)
                .map_err(|e| EmbeddingError::Internal(format!("CPU EP: {}", e)))
        }
    }
}
```

---

### Phase 3: Testing & Benchmarking (6-8 hours)

#### Task 3.1: Integration Tests

**File**: `crates/akidb-embedding/tests/onnx_coreml_tests.rs`

```rust
#[tokio::test]
async fn test_onnx_coreml_initialization() {
    let provider = OnnxEmbeddingProvider::new(
        "models/qwen3-embedding-0.6b/model.onnx",
        "Qwen3-Embedding-0.6B"
    ).await.expect("Failed to initialize");

    let info = provider.model_info().await.expect("model_info failed");
    assert_eq!(info.dimension, 768);
    assert_eq!(info.model, "Qwen3-Embedding-0.6B");
}

#[tokio::test]
async fn test_onnx_coreml_single_embedding() {
    let provider = OnnxEmbeddingProvider::new(
        "models/qwen3-embedding-0.6b/model.onnx",
        "Qwen3-Embedding-0.6B"
    ).await.expect("Failed to initialize");

    let request = BatchEmbeddingRequest {
        model: "Qwen3-Embedding-0.6B".to_string(),
        inputs: vec!["Hello, world!".to_string()],
    };

    let response = provider.embed_batch(request).await.expect("embed_batch failed");

    assert_eq!(response.embeddings.len(), 1);
    assert_eq!(response.embeddings[0].len(), 768);

    // Verify L2 normalization
    let norm: f32 = response.embeddings[0].iter().map(|x| x * x).sum::<f32>().sqrt();
    assert!((norm - 1.0).abs() < 0.01, "Embedding not normalized: norm={}", norm);
}

#[tokio::test]
async fn test_onnx_coreml_batch_embedding() {
    let provider = OnnxEmbeddingProvider::new(
        "models/qwen3-embedding-0.6b/model.onnx",
        "Qwen3-Embedding-0.6B"
    ).await.expect("Failed to initialize");

    let request = BatchEmbeddingRequest {
        model: "Qwen3-Embedding-0.6B".to_string(),
        inputs: vec![
            "Apple Silicon M1 processor".to_string(),
            "Neural Engine accelerates ML workloads".to_string(),
            "CoreML framework for Mac".to_string(),
        ],
    };

    let response = provider.embed_batch(request).await.expect("embed_batch failed");

    assert_eq!(response.embeddings.len(), 3);
    for embedding in &response.embeddings {
        assert_eq!(embedding.len(), 768);
    }
}
```

#### Task 3.2: Performance Benchmarks

**File**: `benches/onnx_coreml_bench.rs`

```rust
use criterion::{criterion_group, criterion_main, Criterion, BenchmarkId};

fn benchmark_onnx_coreml(c: &mut Criterion) {
    let runtime = tokio::runtime::Runtime::new().unwrap();
    let provider = runtime.block_on(async {
        OnnxEmbeddingProvider::new(
            "models/qwen3-embedding-0.6b/model.onnx",
            "Qwen3-Embedding-0.6B"
        ).await.unwrap()
    });

    let mut group = c.benchmark_group("onnx_coreml");

    // Single text
    group.bench_function(BenchmarkId::new("single", "short"), |b| {
        b.to_async(&runtime).iter(|| async {
            let request = BatchEmbeddingRequest {
                model: "Qwen3-Embedding-0.6B".to_string(),
                inputs: vec!["Hello world".to_string()],
            };
            provider.embed_batch(request).await.unwrap()
        });
    });

    // Batch of 8
    group.bench_function(BenchmarkId::new("batch", "8"), |b| {
        b.to_async(&runtime).iter(|| async {
            let request = BatchEmbeddingRequest {
                model: "Qwen3-Embedding-0.6B".to_string(),
                inputs: vec!["test text".to_string(); 8],
            };
            provider.embed_batch(request).await.unwrap()
        });
    });

    group.finish();
}

criterion_group!(benches, benchmark_onnx_coreml);
criterion_main!(benches);
```

**Run Benchmarks**:
```bash
cargo bench --features onnx --bench onnx_coreml_bench
```

**Expected Performance** (Mac M1/M2/M3):
- Single text: **<20ms** (CoreML GPU/ANE)
- Batch of 8: **<60ms** (CoreML GPU/ANE)
- Batch of 32: **<180ms** (CoreML GPU/ANE)

---

### Phase 4: Documentation & Deployment (4-6 hours)

#### Task 4.1: Update README

**File**: `crates/akidb-embedding/README.md`

```markdown
# AkiDB Embedding Service

Provides text embedding generation with Mac ARM GPU acceleration via ONNX Runtime + CoreML.

## Features

- **ONNX Runtime with CoreML EP**: Mac ARM GPU/ANE acceleration
- **Qwen3-Embedding-0.6B**: 768-dimensional embeddings
- **Async API**: Tokio-based async/await
- **Batch Processing**: Up to 32 texts per request

## Quick Start

```rust
use akidb_embedding::OnnxEmbeddingProvider;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let provider = OnnxEmbeddingProvider::new(
        "models/qwen3-embedding-0.6b/model.onnx",
        "Qwen3-Embedding-0.6B"
    ).await?;

    let request = BatchEmbeddingRequest {
        model: "Qwen3-Embedding-0.6B".to_string(),
        inputs: vec!["Hello, world!".to_string()],
    };

    let response = provider.embed_batch(request).await?;
    println!("Embedding: {:?}", response.embeddings[0]);

    Ok(())
}
```

## Performance

| Batch Size | Latency (P95) | Throughput |
|------------|---------------|------------|
| 1 | <20ms | 50+ QPS |
| 8 | <60ms | 130+ QPS |
| 32 | <180ms | 180+ QPS |

Tested on: Mac M1 Pro, macOS 15.1, CoreML EP enabled
```

#### Task 4.2: Create Model Download Script

**File**: `scripts/download_qwen3_onnx.py`

```python
#!/usr/bin/env python3
"""Download Qwen3-Embedding-0.6B ONNX model from Hugging Face."""

from huggingface_hub import snapshot_download
import argparse

def download_qwen3_onnx(output_dir="models/qwen3-embedding-0.6b"):
    print(f"ğŸ“¥ Downloading Qwen3-Embedding-0.6B ONNX to {output_dir}...")

    snapshot_download(
        repo_id="Alibaba-NLP/new-impl/Qwen3-Embedding-0.6B-ONNX",
        local_dir=output_dir,
        repo_type="model"
    )

    print(f"âœ… Download complete: {output_dir}")
    print(f"\nFiles:")
    import os
    for file in os.listdir(output_dir):
        size_mb = os.path.getsize(os.path.join(output_dir, file)) / 1024 / 1024
        print(f"  - {file} ({size_mb:.1f} MB)")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--output", default="models/qwen3-embedding-0.6b")
    args = parser.parse_args()

    download_qwen3_onnx(args.output)
```

---

## Risk Mitigation

### Risk 1: ONNX Model Export Issues

**Issue**: Qwen3-Embedding ONNX export reported issues (Summer 2025)

**Mitigation**:
1. Use community-verified ONNX model from HuggingFace
2. Validate with `onnx.checker.check_model()`
3. Test with onnxruntime Python before Rust implementation
4. Document model version and SHA256 checksum

**Fallback**: If community ONNX model has issues, export manually with updated optimum/transformers

### Risk 2: CoreML EP Operator Support

**Issue**: Some ONNX operators may not be supported by CoreML EP

**Mitigation**:
1. Test with onnxruntime Python first (identifies unsupported ops)
2. Implement graceful fallback to CPU EP
3. Log warnings when CoreML EP unavailable
4. Document supported operator list

**Fallback**: Use CPU EP (slower but compatible)

### Risk 3: macOS 15 CoreML Stability

**Issue**: macOS 15 early releases had CoreML EP crashes (now fixed)

**Mitigation**:
1. Require macOS 15.1+ in documentation
2. Test on multiple macOS versions
3. Implement crash recovery (restart session on failure)
4. Monitor Apple's ONNX Runtime releases

**Fallback**: Recommend macOS upgrade or use CPU EP

### Risk 4: Static vs Dynamic Input Shapes

**Issue**: CoreML EP may require static input shapes

**Mitigation**:
1. Set `require_static_input_shapes: false` in CoreML options
2. Pad all inputs to fixed length (e.g., 512 tokens)
3. Test with varying input lengths
4. Document max sequence length

**Fallback**: Use fixed 512-token padding (standard BERT approach)

---

## Success Criteria

### Functional Requirements
- [x] ONNX model loads successfully
- [x] CoreML EP activates (verified in logs)
- [ ] Tokenization works correctly
- [ ] Inference produces 768-dim embeddings
- [ ] Embeddings are L2 normalized (norm â‰ˆ 1.0)
- [ ] Health check passes
- [ ] Batch processing works (1-32 texts)

### Performance Requirements
- [ ] Single text: P95 <20ms (CoreML GPU/ANE)
- [ ] Batch of 8: P95 <60ms
- [ ] Batch of 32: P95 <180ms
- [ ] Throughput: >50 QPS (single text)

### Quality Requirements
- [ ] 10+ integration tests passing
- [ ] Benchmarks documented
- [ ] Error handling for CoreML failure
- [ ] Graceful CPU fallback
- [ ] README with examples

### Deployment Requirements
- [ ] Model download script works
- [ ] Cargo build succeeds with `--features onnx`
- [ ] No Python dependency (pure Rust)
- [ ] Documentation complete

---

## Timeline

**Total Estimated Time**: 22-32 hours (3-4 days)

| Phase | Tasks | Estimated Time | Status |
|-------|-------|----------------|--------|
| **Phase 1** | Model acquisition & validation | 4-6 hours | â³ Pending |
| **Phase 2** | Rust implementation | 8-12 hours | â³ Pending |
| **Phase 3** | Testing & benchmarking | 6-8 hours | â³ Pending |
| **Phase 4** | Documentation & deployment | 4-6 hours | â³ Pending |

---

## Next Steps

1. **Download Qwen3-Embedding ONNX model** (Phase 1)
2. **Validate with Python onnxruntime** (Phase 1)
3. **Update onnx.rs with CoreML EP** (Phase 2)
4. **Implement inference pipeline** (Phase 2)
5. **Write integration tests** (Phase 3)
6. **Run benchmarks** (Phase 3)
7. **Update documentation** (Phase 4)

---

## References

**Model**:
- [Qwen3-Embedding-0.6B-ONNX](https://huggingface.co/Alibaba-NLP/new-impl/Qwen3-Embedding-0.6B-ONNX)

**ONNX Runtime**:
- [ort crate documentation](https://docs.rs/ort/latest/ort/)
- [CoreML EP configuration](https://onnxruntime.ai/docs/execution-providers/CoreML-ExecutionProvider.html)

**Related Docs**:
- [Candle Metal Investigation](../tmp/CANDLE-METAL-INVESTIGATION.md)
- Original export script: `scripts/export_onnx_model.py`

---

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
