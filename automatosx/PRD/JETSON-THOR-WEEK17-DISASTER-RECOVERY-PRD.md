# Week 17 PRD: Disaster Recovery & Business Continuity

**Project:** AkiDB 2.0 - Jetson Thor Optimization - Week 17
**Focus:** Disaster Recovery, Backup/Restore, Multi-Region Failover, Business Continuity
**Timeline:** 5 days (November 24-28, 2025)
**Status:** Planning

---

## Executive Summary

After Week 16's security hardening (SOC 2/GDPR/HIPAA ready), Week 17 focuses on **disaster recovery and business continuity** to achieve enterprise-grade reliability with 99.99% uptime SLA (52.6 minutes downtime/year).

### Strategic Context

**Current State (Week 16):**
- âœ… Security hardened (100% encryption, zero-trust, SOC 2 ready)
- âœ… High performance (P95 <25ms globally)
- âœ… Cost optimized ($3,520/month)
- âœ… Observability ready (MTTD <5min, MTTR <15min)
- âŒ **Single region deployment (no disaster recovery)**
- âŒ **No automated backup/restore**
- âŒ **No multi-region failover**
- âŒ **RPO/RTO undefined (data loss risk)**

**Business Driver:**
Enterprise SLAs require:
1. **99.99% uptime** (52.6 min downtime/year) â†’ Multi-region active-active
2. **RPO <15 minutes** (Recovery Point Objective) â†’ Continuous backup
3. **RTO <30 minutes** (Recovery Time Objective) â†’ Automated failover
4. **Zero data loss** for critical operations â†’ Synchronous replication
5. **Annual DR drills** â†’ Automated testing and validation

**Week 17 Goals:**
- Deploy multi-region active-active architecture (US-East, US-West, EU-West)
- Implement automated backup/restore with <15min RPO
- Enable automated failover with <30min RTO
- Deploy chaos engineering for DR validation
- Achieve 99.99% uptime SLA capability

---

## Goals & Non-Goals

### Goals (P0 - Must Have)
1. **Multi-Region Active-Active**
   - âœ… Deploy AkiDB in 3 regions (US-East-1, US-West-2, EU-West-1)
   - âœ… Implement cross-region data replication
   - âœ… Enable intelligent traffic routing (Route 53 latency-based)
   - âœ… Synchronize metadata across regions

2. **Automated Backup & Restore**
   - âœ… Continuous incremental backups (every 5 minutes)
   - âœ… Point-in-time recovery (PITR) capability
   - âœ… Cross-region backup replication
   - âœ… Automated restore testing (weekly)

3. **Disaster Recovery Automation**
   - âœ… Automated failover (health check-based)
   - âœ… RTO <30 minutes (automated recovery)
   - âœ… RPO <15 minutes (maximum data loss)
   - âœ… Failback procedures (return to primary)

4. **Chaos Engineering for DR**
   - âœ… Region failure simulation
   - âœ… Data corruption recovery testing
   - âœ… Network partition testing
   - âœ… Cascading failure scenarios

### Goals (P1 - Should Have)
- GitOps-based infrastructure as code (IaC) for all regions
- Automated DR runbooks with one-click recovery
- Backup retention policies (7 days hot, 30 days warm, 1 year cold)
- Cost optimization for cross-region traffic

### Goals (P2 - Nice to Have)
- Active-active-active (3 regions simultaneously writable)
- Global load balancing with Cloudflare
- Automated compliance for DR (SOC 2 BC/DR controls)

### Non-Goals
- âŒ On-premises disaster recovery site
- âŒ Physical data center failover
- âŒ Manual failover procedures (must be automated)
- âŒ Cold standby (active-active only)

---

## Week 16 Baseline Analysis

### Current Architecture Limitations

**Single-Region Deployment:**
- All infrastructure in US-East-1
- Single point of failure (regional outage = complete service unavailability)
- No geographic redundancy
- RTO: Unknown (manual recovery)
- RPO: Unknown (no backups)

**Backup Gaps:**
| Component | Current Backup | Desired State |
|-----------|---------------|---------------|
| **SQLite metadata** | None | Continuous + PITR |
| **Vector embeddings (S3)** | S3 versioning only | Cross-region replication |
| **HNSW indices** | Rebuilt on restart | Snapshot + incremental |
| **Vault secrets** | Raft snapshots (manual) | Automated cross-region |
| **Kubernetes state** | etcd backup (manual) | Velero automated |

**Disaster Scenarios (Unprotected):**

| Scenario | Current Impact | Target Impact |
|----------|---------------|---------------|
| **AWS Region Outage** | 100% service unavailability | <1% traffic loss (failover to other regions) |
| **Data Corruption** | Permanent data loss | Restore from last backup (<15min data loss) |
| **Accidental Deletion** | Permanent loss | Restore from point-in-time |
| **Ransomware Attack** | Encrypted data unusable | Restore from immutable backup |
| **Network Partition** | Split-brain data corruption | Automated conflict resolution |

---

## Week 17 Architecture: Multi-Region Active-Active

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Global Architecture (3 Regions)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚   Route 53 (Global)   â”‚                        â”‚
â”‚                    â”‚  Latency-based routing â”‚                        â”‚
â”‚                    â”‚  Health checks every 30sâ”‚                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                            â”‚                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚              â”‚             â”‚             â”‚                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚  US-East-1    â”‚  â”‚  US-West-2  â”‚  â”‚  EU-West-1    â”‚          â”‚
â”‚    â”‚  (Primary)    â”‚  â”‚  (Secondary)â”‚  â”‚  (EU-Primary) â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                       â”‚
â”‚  Each Region Contains:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Region: US-East-1                                            â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚
â”‚  â”‚  â”‚  EKS Cluster   â”‚     â”‚  RDS Aurora    â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚  (akidb-pods)  â”‚â”€â”€â”€â”€â–¶â”‚  (metadata)    â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚  3 AZs         â”‚     â”‚  Multi-AZ      â”‚                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚
â”‚  â”‚         â”‚                       â”‚                             â”‚  â”‚
â”‚  â”‚         â”‚                       â”‚                             â”‚  â”‚
â”‚  â”‚         â–¼                       â–¼                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚
â”‚  â”‚  â”‚  S3 Regional   â”‚â—„â”€â”€â”€â”€â”‚ Cross-Region   â”‚                  â”‚  â”‚
â”‚  â”‚  â”‚  (embeddings)  â”‚     â”‚  Replication   â”‚                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚
â”‚  â”‚         â”‚                       â”‚                             â”‚  â”‚
â”‚  â”‚         â”‚                       â”‚                             â”‚  â”‚
â”‚  â”‚         â–¼                       â–¼                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚  â”‚
â”‚  â”‚  â”‚  Velero Backup                         â”‚                 â”‚  â”‚
â”‚  â”‚  â”‚  - K8s resources (every 6 hours)      â”‚                 â”‚  â”‚
â”‚  â”‚  â”‚  - PVC snapshots (every 5 minutes)    â”‚                 â”‚  â”‚
â”‚  â”‚  â”‚  - Metadata exports (every 5 minutes) â”‚                 â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â”‚  Cross-Region Data Flow:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  US-East-1 (Write)  â”€â”€â”                                       â”‚  â”‚
â”‚  â”‚                       â”‚                                        â”‚  â”‚
â”‚  â”‚                       â”œâ”€â”€â–¶ Aurora Global Database             â”‚  â”‚
â”‚  â”‚                       â”‚    (Async replication <1s lag)        â”‚  â”‚
â”‚  â”‚                       â”‚                                        â”‚  â”‚
â”‚  â”‚  US-West-2 (Read) â—„â”€â”€â”€â”¤                                       â”‚  â”‚
â”‚  â”‚  EU-West-1 (Read) â—„â”€â”€â”€â”˜                                       â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  S3 Cross-Region Replication:                                 â”‚  â”‚
â”‚  â”‚  US-East-1 â†’ US-West-2 (async, ~15min lag)                   â”‚  â”‚
â”‚  â”‚  US-East-1 â†’ EU-West-1 (async, ~15min lag)                   â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â”‚  Disaster Recovery Flow:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  1. Route 53 detects US-East-1 unhealthy (3 failed checks)   â”‚  â”‚
â”‚  â”‚  2. Traffic automatically rerouted to US-West-2              â”‚  â”‚
â”‚  â”‚  3. Aurora promotes US-West-2 to primary (< 1 minute)        â”‚  â”‚
â”‚  â”‚  4. S3 switches to US-West-2 bucket (eventual consistency)   â”‚  â”‚
â”‚  â”‚  5. PagerDuty alert: "Region failover triggered"             â”‚  â”‚
â”‚  â”‚  6. RTO achieved: <30 minutes                                 â”‚  â”‚
â”‚  â”‚  7. RPO achieved: <15 minutes (Aurora lag + S3 replication)  â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

**1. Route 53 Global Traffic Management**
- Latency-based routing (route to nearest healthy region)
- Health checks every 30 seconds
- Automated failover on 3 consecutive failures
- Weighted routing for A/B testing

**2. Aurora Global Database**
- Primary region: US-East-1 (read-write)
- Secondary regions: US-West-2, EU-West-1 (read-only)
- Async replication lag: <1 second
- Automatic failover promotion: <1 minute
- Cross-region backup replication

**3. S3 Cross-Region Replication**
- Real-time replication for new objects
- Batch replication for existing objects
- Replication lag: ~15 minutes
- Versioning enabled for rollback
- Object Lock for immutable backups

**4. Velero Kubernetes Backup**
- Scheduled backups every 6 hours (K8s resources)
- Volume snapshots every 5 minutes (PVCs)
- Cross-region backup storage
- Automated restore testing weekly

**5. HashiCorp Vault Disaster Recovery**
- Raft snapshots every 1 hour
- Cross-region snapshot replication
- Automated failover to secondary Vault cluster
- Unseal key backup in AWS Secrets Manager

---

## Technical Implementation

### 1. Multi-Region EKS Cluster Deployment

#### 1.1 Deploy EKS in US-West-2

```bash
# Create EKS cluster in US-West-2
eksctl create cluster \
    --name akidb-cluster-us-west-2 \
    --region us-west-2 \
    --version 1.28 \
    --nodegroup-name akidb-nodes \
    --node-type t4g.medium \
    --nodes 3 \
    --nodes-min 3 \
    --nodes-max 10 \
    --with-oidc \
    --ssh-access \
    --ssh-public-key ~/.ssh/id_rsa.pub \
    --managed \
    --asg-access \
    --full-ecr-access \
    --alb-ingress-access \
    --appmesh-access

# Install Istio in US-West-2
istioctl install --set profile=production -y --set values.global.multiCluster.clusterName=akidb-cluster-us-west-2

# Install cert-manager
helm install cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --version v1.13.2 \
    --set installCRDs=true

# Install Velero for backups
helm install velero vmware-tanzu/velero \
    --namespace velero \
    --create-namespace \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.bucket=akidb-backups-us-west-2 \
    --set configuration.backupStorageLocation.config.region=us-west-2 \
    --set configuration.volumeSnapshotLocation.config.region=us-west-2 \
    --set serviceAccount.server.annotations."eks\.amazonaws\.com/role-arn"=arn:aws:iam::ACCOUNT_ID:role/VeleroRole \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=velero/velero-plugin-for-aws:v1.8.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins
```

#### 1.2 Deploy EKS in EU-West-1

```bash
# Repeat similar eksctl create cluster command for EU-West-1
eksctl create cluster \
    --name akidb-cluster-eu-west-1 \
    --region eu-west-1 \
    --version 1.28 \
    --nodegroup-name akidb-nodes \
    --node-type t4g.medium \
    --nodes 3 \
    --nodes-min 3 \
    --nodes-max 10 \
    --with-oidc \
    --managed

# Install Istio, cert-manager, Velero (same as US-West-2)
```

### 2. Aurora Global Database Setup

#### 2.1 Create Aurora Global Database

```bash
# Create primary Aurora cluster in US-East-1
aws rds create-db-cluster \
    --db-cluster-identifier akidb-metadata-primary \
    --engine aurora-postgresql \
    --engine-version 15.4 \
    --master-username akidb_admin \
    --master-user-password "$(aws secretsmanager get-secret-value --secret-id akidb/rds/password --query SecretString --output text)" \
    --database-name akidb_metadata \
    --db-subnet-group-name akidb-subnet-group \
    --vpc-security-group-ids sg-0123456789abcdef0 \
    --storage-encrypted \
    --kms-key-id arn:aws:kms:us-east-1:ACCOUNT_ID:key/KEY_ID \
    --backup-retention-period 7 \
    --preferred-backup-window 03:00-04:00 \
    --region us-east-1

# Create primary instance
aws rds create-db-instance \
    --db-instance-identifier akidb-metadata-primary-instance-1 \
    --db-cluster-identifier akidb-metadata-primary \
    --db-instance-class db.r6g.large \
    --engine aurora-postgresql \
    --region us-east-1

# Create Global Database
aws rds create-global-cluster \
    --global-cluster-identifier akidb-metadata-global \
    --source-db-cluster-identifier arn:aws:rds:us-east-1:ACCOUNT_ID:cluster:akidb-metadata-primary \
    --region us-east-1

# Add US-West-2 secondary region
aws rds create-db-cluster \
    --db-cluster-identifier akidb-metadata-secondary-us-west-2 \
    --engine aurora-postgresql \
    --engine-version 15.4 \
    --global-cluster-identifier akidb-metadata-global \
    --region us-west-2

aws rds create-db-instance \
    --db-instance-identifier akidb-metadata-secondary-us-west-2-instance-1 \
    --db-cluster-identifier akidb-metadata-secondary-us-west-2 \
    --db-instance-class db.r6g.large \
    --engine aurora-postgresql \
    --region us-west-2

# Add EU-West-1 secondary region
aws rds create-db-cluster \
    --db-cluster-identifier akidb-metadata-secondary-eu-west-1 \
    --engine aurora-postgresql \
    --engine-version 15.4 \
    --global-cluster-identifier akidb-metadata-global \
    --region eu-west-1

aws rds create-db-instance \
    --db-instance-identifier akidb-metadata-secondary-eu-west-1-instance-1 \
    --db-cluster-identifier akidb-metadata-secondary-eu-west-1 \
    --db-instance-class db.r6g.large \
    --engine aurora-postgresql \
    --region eu-west-1
```

#### 2.2 Configure Connection Pooling

```rust
// Rust code: Multi-region Aurora connection with failover
use sqlx::{postgres::PgPoolOptions, PgPool};
use std::time::Duration;

pub async fn create_multi_region_pool() -> Result<PgPool, sqlx::Error> {
    let primary_url = "postgresql://akidb_admin:PASSWORD@akidb-metadata-primary.cluster-xxx.us-east-1.rds.amazonaws.com:5432/akidb_metadata";
    let secondary_us_url = "postgresql://akidb_admin:PASSWORD@akidb-metadata-secondary-us-west-2.cluster-yyy.us-west-2.rds.amazonaws.com:5432/akidb_metadata";
    let secondary_eu_url = "postgresql://akidb_admin:PASSWORD@akidb-metadata-secondary-eu-west-1.cluster-zzz.eu-west-1.rds.amazonaws.com:5432/akidb_metadata";

    // Try primary first
    let pool = PgPoolOptions::new()
        .max_connections(20)
        .acquire_timeout(Duration::from_secs(5))
        .connect(primary_url)
        .await;

    match pool {
        Ok(pool) => Ok(pool),
        Err(e) => {
            tracing::warn!("Primary Aurora unavailable, trying US-West-2: {}", e);

            // Fallback to US-West-2
            let pool = PgPoolOptions::new()
                .max_connections(20)
                .acquire_timeout(Duration::from_secs(5))
                .connect(secondary_us_url)
                .await;

            match pool {
                Ok(pool) => Ok(pool),
                Err(e) => {
                    tracing::warn!("US-West-2 Aurora unavailable, trying EU-West-1: {}", e);

                    // Last resort: EU-West-1
                    PgPoolOptions::new()
                        .max_connections(20)
                        .acquire_timeout(Duration::from_secs(5))
                        .connect(secondary_eu_url)
                        .await
                }
            }
        }
    }
}
```

### 3. S3 Cross-Region Replication

#### 3.1 Configure S3 CRR

```bash
# Enable versioning on source bucket (required for CRR)
aws s3api put-bucket-versioning \
    --bucket akidb-embeddings-us-east-1 \
    --versioning-configuration Status=Enabled \
    --region us-east-1

# Create destination buckets
aws s3api create-bucket \
    --bucket akidb-embeddings-us-west-2 \
    --region us-west-2 \
    --create-bucket-configuration LocationConstraint=us-west-2

aws s3api put-bucket-versioning \
    --bucket akidb-embeddings-us-west-2 \
    --versioning-configuration Status=Enabled \
    --region us-west-2

aws s3api create-bucket \
    --bucket akidb-embeddings-eu-west-1 \
    --region eu-west-1 \
    --create-bucket-configuration LocationConstraint=eu-west-1

aws s3api put-bucket-versioning \
    --bucket akidb-embeddings-eu-west-1 \
    --versioning-configuration Status=Enabled \
    --region eu-west-1

# Create IAM role for replication
cat <<EOF > s3-replication-trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": {"Service": "s3.amazonaws.com"},
    "Action": "sts:AssumeRole"
  }]
}
EOF

aws iam create-role \
    --role-name S3ReplicationRole \
    --assume-role-policy-document file://s3-replication-trust-policy.json

# Attach replication policy
cat <<EOF > s3-replication-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetReplicationConfiguration",
        "s3:ListBucket"
      ],
      "Resource": "arn:aws:s3:::akidb-embeddings-us-east-1"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObjectVersionForReplication",
        "s3:GetObjectVersionAcl"
      ],
      "Resource": "arn:aws:s3:::akidb-embeddings-us-east-1/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:ReplicateObject",
        "s3:ReplicateDelete"
      ],
      "Resource": [
        "arn:aws:s3:::akidb-embeddings-us-west-2/*",
        "arn:aws:s3:::akidb-embeddings-eu-west-1/*"
      ]
    }
  ]
}
EOF

aws iam put-role-policy \
    --role-name S3ReplicationRole \
    --policy-name S3ReplicationPolicy \
    --policy-document file://s3-replication-policy.json

# Configure replication
cat <<EOF > s3-replication-config.json
{
  "Role": "arn:aws:iam::ACCOUNT_ID:role/S3ReplicationRole",
  "Rules": [
    {
      "ID": "ReplicateToUSWest2",
      "Priority": 1,
      "Filter": {},
      "Status": "Enabled",
      "Destination": {
        "Bucket": "arn:aws:s3:::akidb-embeddings-us-west-2",
        "ReplicationTime": {
          "Status": "Enabled",
          "Time": {"Minutes": 15}
        },
        "Metrics": {
          "Status": "Enabled",
          "EventThreshold": {"Minutes": 15}
        }
      },
      "DeleteMarkerReplication": {"Status": "Enabled"}
    },
    {
      "ID": "ReplicateToEUWest1",
      "Priority": 2,
      "Filter": {},
      "Status": "Enabled",
      "Destination": {
        "Bucket": "arn:aws:s3:::akidb-embeddings-eu-west-1",
        "ReplicationTime": {
          "Status": "Enabled",
          "Time": {"Minutes": 15}
        },
        "Metrics": {
          "Status": "Enabled",
          "EventThreshold": {"Minutes": 15}
        }
      },
      "DeleteMarkerReplication": {"Status": "Enabled"}
    }
  ]
}
EOF

aws s3api put-bucket-replication \
    --bucket akidb-embeddings-us-east-1 \
    --replication-configuration file://s3-replication-config.json \
    --region us-east-1
```

### 4. Route 53 Global Traffic Management

#### 4.1 Create Health Checks

```bash
# Health check for US-East-1
aws route53 create-health-check \
    --caller-reference "akidb-us-east-1-$(date +%s)" \
    --health-check-config \
        Type=HTTPS,\
        ResourcePath=/health,\
        FullyQualifiedDomainName=api-us-east-1.akidb.com,\
        Port=443,\
        RequestInterval=30,\
        FailureThreshold=3,\
        MeasureLatency=true \
    --region us-east-1

# Health check for US-West-2
aws route53 create-health-check \
    --caller-reference "akidb-us-west-2-$(date +%s)" \
    --health-check-config \
        Type=HTTPS,\
        ResourcePath=/health,\
        FullyQualifiedDomainName=api-us-west-2.akidb.com,\
        Port=443,\
        RequestInterval=30,\
        FailureThreshold=3,\
        MeasureLatency=true \
    --region us-east-1

# Health check for EU-West-1
aws route53 create-health-check \
    --caller-reference "akidb-eu-west-1-$(date +%s)" \
    --health-check-config \
        Type=HTTPS,\
        ResourcePath=/health,\
        FullyQualifiedDomainName=api-eu-west-1.akidb.com,\
        Port=443,\
        RequestInterval=30,\
        FailureThreshold=3,\
        MeasureLatency=true \
    --region us-east-1
```

#### 4.2 Configure Latency-Based Routing

```bash
# Get hosted zone ID
HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name \
    --dns-name akidb.com \
    --query 'HostedZones[0].Id' \
    --output text | cut -d'/' -f3)

# Create record set for US-East-1
cat <<EOF > us-east-1-record.json
{
  "Changes": [{
    "Action": "CREATE",
    "ResourceRecordSet": {
      "Name": "api.akidb.com",
      "Type": "A",
      "SetIdentifier": "US-East-1",
      "Region": "us-east-1",
      "HealthCheckId": "HEALTH_CHECK_ID_US_EAST_1",
      "AliasTarget": {
        "HostedZoneId": "Z35SXDOTRQ7X7K",
        "DNSName": "akidb-alb-us-east-1-123456789.us-east-1.elb.amazonaws.com",
        "EvaluateTargetHealth": true
      }
    }
  }]
}
EOF

aws route53 change-resource-record-sets \
    --hosted-zone-id $HOSTED_ZONE_ID \
    --change-batch file://us-east-1-record.json

# Repeat for US-West-2 and EU-West-1
```

### 5. Velero Backup Automation

#### 5.1 Configure Scheduled Backups

```yaml
# Backup schedule for Kubernetes resources
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: akidb-k8s-backup
  namespace: velero
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  template:
    includedNamespaces:
      - akidb
      - istio-system
      - cert-manager
      - vault
    includedResources:
      - '*'
    storageLocation: default
    volumeSnapshotLocations:
      - default
    ttl: 720h  # 30 days

---
# Backup schedule for PVCs (more frequent)
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: akidb-pvc-backup
  namespace: velero
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  template:
    includedNamespaces:
      - akidb
    includedResources:
      - persistentvolumeclaims
      - persistentvolumes
    snapshotVolumes: true
    storageLocation: default
    volumeSnapshotLocations:
      - default
    ttl: 168h  # 7 days
```

#### 5.2 Automated Restore Testing

```bash
# Create weekly restore test CronJob
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata:
  name: velero-restore-test
  namespace: velero
spec:
  schedule: "0 2 * * 0"  # Every Sunday at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: velero
          containers:
            - name: restore-test
              image: velero/velero:v1.12.0
              command:
                - /bin/sh
                - -c
                - |
                  # Get latest backup
                  LATEST_BACKUP=\$(velero backup get --output json | jq -r '.items | sort_by(.status.completionTimestamp) | last | .metadata.name')

                  # Create test namespace
                  kubectl create namespace akidb-restore-test

                  # Restore to test namespace
                  velero restore create \
                    --from-backup \$LATEST_BACKUP \
                    --namespace-mappings akidb:akidb-restore-test \
                    --wait

                  # Validate restore
                  kubectl get pods -n akidb-restore-test

                  # Cleanup
                  kubectl delete namespace akidb-restore-test

                  echo "âœ… Weekly restore test completed successfully"
              env:
                - name: AWS_REGION
                  value: us-east-1
          restartPolicy: OnFailure
EOF
```

### 6. Disaster Recovery Automation

#### 6.1 Automated Failover Lambda

```python
# Lambda function for automated regional failover
import boto3
import json
from datetime import datetime

route53 = boto3.client('route53')
rds = boto3.client('rds')
sns = boto3.client('sns')

def lambda_handler(event, context):
    """
    Triggered by Route 53 health check alarm.
    Performs automated failover to secondary region.
    """

    # Parse health check alarm
    alarm_name = event['Records'][0]['Sns']['Subject']
    alarm_message = json.loads(event['Records'][0]['Sns']['Message'])

    failed_region = extract_region_from_alarm(alarm_name)

    print(f"âš ï¸  Failover triggered for region: {failed_region}")

    # Step 1: Promote Aurora secondary to primary
    if failed_region == 'us-east-1':
        target_region = 'us-west-2'
        target_cluster = 'akidb-metadata-secondary-us-west-2'
    else:
        target_region = 'eu-west-1'
        target_cluster = 'akidb-metadata-secondary-eu-west-1'

    print(f"Promoting Aurora cluster {target_cluster} to primary...")

    rds_target = boto3.client('rds', region_name=target_region)
    rds_target.failover_global_cluster(
        GlobalClusterIdentifier='akidb-metadata-global',
        TargetDbClusterIdentifier=target_cluster
    )

    print(f"âœ… Aurora promoted to {target_region}")

    # Step 2: Update Route 53 weights (drain failed region)
    hosted_zone_id = 'Z1234567890ABC'

    route53.change_resource_record_sets(
        HostedZoneId=hosted_zone_id,
        ChangeBatch={
            'Changes': [
                {
                    'Action': 'UPSERT',
                    'ResourceRecordSet': {
                        'Name': 'api.akidb.com',
                        'Type': 'A',
                        'SetIdentifier': failed_region,
                        'Weight': 0,  # Drain traffic
                        'AliasTarget': {
                            'HostedZoneId': 'Z35SXDOTRQ7X7K',
                            'DNSName': f'akidb-alb-{failed_region}-123.elb.amazonaws.com',
                            'EvaluateTargetHealth': False
                        }
                    }
                }
            ]
        }
    )

    print(f"âœ… Route 53 traffic drained from {failed_region}")

    # Step 3: Notify team
    sns.publish(
        TopicArn='arn:aws:sns:us-east-1:ACCOUNT_ID:akidb-incidents',
        Subject=f'ðŸš¨ Disaster Recovery Failover: {failed_region} â†’ {target_region}',
        Message=f"""
Automated failover completed at {datetime.utcnow().isoformat()}

Failed Region: {failed_region}
Target Region: {target_region}

Actions Taken:
1. âœ… Aurora promoted to {target_region}
2. âœ… Route 53 traffic redirected
3. â³ S3 replication lag: ~15 minutes

RTO: <30 minutes (automated)
RPO: <15 minutes (replication lag)

Next Steps:
1. Investigate root cause of {failed_region} failure
2. Monitor {target_region} for stability
3. Plan failback to {failed_region} when healthy
        """
    )

    return {
        'statusCode': 200,
        'body': json.dumps({
            'failed_region': failed_region,
            'target_region': target_region,
            'timestamp': datetime.utcnow().isoformat()
        })
    }

def extract_region_from_alarm(alarm_name):
    """Extract region from alarm name"""
    if 'us-east-1' in alarm_name:
        return 'us-east-1'
    elif 'us-west-2' in alarm_name:
        return 'us-west-2'
    else:
        return 'eu-west-1'
```

### 7. Chaos Engineering for DR Validation

#### 7.1 Region Failure Simulation

```bash
# Install Chaos Mesh for Kubernetes chaos engineering
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm repo update

helm install chaos-mesh chaos-mesh/chaos-mesh \
    --namespace chaos-mesh \
    --create-namespace \
    --version 2.6.0

# Create region failure experiment
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: simulate-region-failure
  namespace: akidb
spec:
  action: partition
  mode: all
  selector:
    namespaces:
      - akidb
    labelSelectors:
      app: akidb-rest
  direction: both
  externalTargets:
    - akidb-metadata-primary.cluster-xxx.us-east-1.rds.amazonaws.com
  duration: 10m
  scheduler:
    cron: "@weekly"
EOF
```

#### 7.2 Data Corruption Recovery Test

```bash
# Simulate data corruption and test PITR recovery
cat <<EOF > test-data-corruption-recovery.sh
#!/bin/bash

set -e

echo "ðŸ§ª Testing Point-in-Time Recovery (PITR)"
echo "=========================================="

# Step 1: Take snapshot timestamp
SNAPSHOT_TIME=\$(date -u +"%Y-%m-%dT%H:%M:%SZ")
echo "Snapshot time: \$SNAPSHOT_TIME"

# Step 2: Write test data
echo "Writing test data..."
curl -X POST http://api.akidb.com/collections/test-collection/documents \
    -H "Content-Type: application/json" \
    -d '{"text": "Test document before corruption", "metadata": {"test": true}}'

sleep 10

# Step 3: Simulate data corruption (delete collection)
echo "Simulating data corruption (deleting collection)..."
curl -X DELETE http://api.akidb.com/collections/test-collection

# Step 4: Perform PITR restore
echo "Performing Point-in-Time Recovery to \$SNAPSHOT_TIME..."
aws rds restore-db-cluster-to-point-in-time \
    --source-db-cluster-identifier akidb-metadata-primary \
    --db-cluster-identifier akidb-metadata-pitr-test \
    --restore-to-time \$SNAPSHOT_TIME \
    --region us-east-1

# Step 5: Wait for restore to complete
echo "Waiting for restore to complete..."
aws rds wait db-cluster-available \
    --db-cluster-identifier akidb-metadata-pitr-test \
    --region us-east-1

# Step 6: Validate restored data
echo "Validating restored data..."
# (Connect to restored cluster and verify test document exists)

# Step 7: Cleanup
echo "Cleaning up test cluster..."
aws rds delete-db-cluster \
    --db-cluster-identifier akidb-metadata-pitr-test \
    --skip-final-snapshot \
    --region us-east-1

echo "âœ… PITR recovery test completed successfully"
EOF

chmod +x test-data-corruption-recovery.sh
./test-data-corruption-recovery.sh
```

---

## Cost Analysis

### Week 17 Disaster Recovery Cost Breakdown

| Component | Monthly Cost | Notes |
|-----------|--------------|-------|
| **Aurora Global Database** | $600 | 3 regions Ã— db.r6g.large ($200/region) |
| **S3 Cross-Region Replication** | $150 | Data transfer + storage in 3 regions |
| **Route 53 Health Checks** | $15 | 3 health checks @ $0.50/check + queries |
| **EKS Clusters (2 additional)** | $146 | 2 regions Ã— $73/month (control plane) |
| **Velero Backup Storage** | $80 | S3 storage for backups (7-day hot + 30-day warm) |
| **EC2 Nodes (US-West-2)** | $150 | 3 Ã— t4g.medium @ $50/month |
| **EC2 Nodes (EU-West-1)** | $150 | 3 Ã— t4g.medium @ $50/month |
| **CloudWatch Alarms (DR)** | $20 | 20 alarms @ $0.10/alarm for health monitoring |
| **Lambda (Failover)** | $5 | Minimal invocations (only during failures) |
| **Data Transfer (Cross-Region)** | $100 | S3 + Aurora replication traffic |
| **Total** | **+$1,416/month** | **40.2% infrastructure increase** |

**Cumulative Infrastructure Cost:**
- Week 16: $3,520/month
- Week 17: $4,936/month (+$1,416)
- **Total increase from Week 8:** -38% (from $8,000 to $4,936)

**Cost Justification:**
- **99.99% uptime SLA:** Enables premium enterprise contracts ($100k+ ARR)
- **Regulatory compliance:** Required for healthcare/finance (HIPAA, SOC 2 BC/DR)
- **Risk mitigation:** Prevents $500k+ revenue loss from regional outages
- **Competitive advantage:** Most competitors offer 99.9% SLA (4.3x better uptime)

**Cost Optimization Opportunities:**
- Use Aurora Serverless v2 for secondary regions (scale to zero during low traffic)
- Reserved Instances for EC2 nodes (30% savings)
- S3 Intelligent-Tiering for backup storage (automatic cost optimization)

---

## Success Criteria

### P0 (Must Have) - 100% Required
- [ ] Multi-region deployment: 3 regions operational (US-East-1, US-West-2, EU-West-1)
- [ ] Aurora Global Database: Async replication <1s lag
- [ ] S3 Cross-Region Replication: <15min lag
- [ ] Route 53 automated failover: <3 failures trigger failover
- [ ] Velero backups: Scheduled every 5 minutes (PVCs), 6 hours (K8s resources)
- [ ] RTO <30 minutes: Automated failover tested
- [ ] RPO <15 minutes: Data loss <15 minutes validated
- [ ] Automated restore testing: Weekly validation passing

### P1 (Should Have) - 80% Target
- [ ] Chaos engineering: 3+ DR scenarios tested
- [ ] Failback procedures: Documented and tested
- [ ] GitOps IaC: Terraform/Pulumi for all regions
- [ ] Backup retention: 7 days hot, 30 days warm, 1 year cold
- [ ] Cost optimization: Reserved Instances purchased

### P2 (Nice to Have) - 50% Target
- [ ] Active-active-active: All 3 regions writable
- [ ] Global load balancing: Cloudflare integration
- [ ] SOC 2 BC/DR compliance: Automated evidence collection

**Overall Success:** All P0 + 80% P1 + 50% P2

---

## Risk Management

### Disaster Recovery Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| **Split-brain during failover** | Medium | Critical | Use Aurora Global Database with managed failover, avoid manual intervention |
| **Data inconsistency across regions** | Medium | High | Monitor replication lag, alert if >5 seconds, test conflict resolution |
| **Failover false positives** | Low | Medium | Require 3 consecutive health check failures (90 seconds) before failover |
| **Cross-region data transfer costs** | High | Medium | Optimize S3 replication (filter unnecessary objects), use S3 Transfer Acceleration |
| **Backup storage costs** | High | Low | Implement tiered retention (7d hot, 30d warm, 1y cold), use S3 Intelligent-Tiering |
| **Untested failover procedures** | Medium | Critical | Automated weekly restore testing, quarterly full DR drills |

---

## Day-by-Day Implementation Plan

### Day 1: Multi-Region Infrastructure Deployment
- Deploy EKS clusters in US-West-2 and EU-West-1
- Install Istio, cert-manager, Velero in new regions
- Deploy akidb-rest and akidb-embedding services
- Validate: All 3 regions serving traffic

### Day 2: Aurora Global Database Setup
- Create Aurora Global Database with US-East-1 primary
- Add US-West-2 and EU-West-1 secondary regions
- Configure read-only endpoints for secondary regions
- Test replication lag (target <1 second)
- Validate: Write to primary, read from secondary with <1s lag

### Day 3: S3 Cross-Region Replication & Route 53
- Configure S3 CRR for embeddings buckets
- Set up Route 53 health checks for all regions
- Configure latency-based routing with failover
- Validate: Objects replicate within 15 minutes, Route 53 routes to nearest region

### Day 4: Velero Backup Automation & DR Lambda
- Deploy Velero scheduled backups (5min PVC, 6hr K8s)
- Create automated failover Lambda function
- Test failover trigger (simulate region failure)
- Validate: Automated failover completes in <30 minutes

### Day 5: Chaos Engineering & DR Validation
- Deploy Chaos Mesh for DR testing
- Run region failure simulation
- Run data corruption recovery test (PITR)
- Generate Week 17 completion report
- Validate: All DR scenarios pass, RTO <30min, RPO <15min

---

## Conclusion

Week 17 establishes **enterprise-grade disaster recovery and business continuity** for AkiDB 2.0, enabling:

âœ… **99.99% uptime SLA** (52.6 min downtime/year) via multi-region active-active
âœ… **RTO <30 minutes** (automated regional failover)
âœ… **RPO <15 minutes** (maximum data loss from Aurora + S3 replication lag)
âœ… **Automated backup/restore** (continuous + point-in-time recovery)
âœ… **Chaos-tested resilience** (weekly DR validation)

**Enterprise Readiness:**
- âœ… Can survive AWS regional outages
- âœ… Can recover from data corruption/deletion
- âœ… Can meet 99.99% SLA commitments
- âœ… Can pass SOC 2 BC/DR controls

**Cost Impact:** +$1,416/month (40.2% increase, justified by $100k+ enterprise SLAs)

**Overall Assessment:** Week 17 transforms AkiDB 2.0 into a **mission-critical, disaster-resilient platform** capable of meeting the most demanding enterprise availability requirements.

**Status:** âœ… Ready for Week 17 execution
