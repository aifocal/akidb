[package]
name = "akidb-embedding"
version.workspace = true
edition.workspace = true
rust-version.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true

[dependencies]
# Async runtime
tokio = { workspace = true }
async-trait = "0.1"

# Synchronization
parking_lot = "0.12"

# Error handling
thiserror = { workspace = true }
anyhow = { workspace = true }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }

# Python integration (optional, gated behind "mlx" feature)
# Python 3.12 required for AkiDB 2.0
# This allows the crate to build on machines without Python 3.12+
# Using abi3-py312 for Python 3.12 stable ABI
pyo3 = { version = "0.22", features = ["auto-initialize", "abi3-py312"], optional = true }

# ONNX Runtime with CoreML EP (primary provider for Mac ARM GPU)
# TensorRT and CUDA features added for Jetson Thor support
ort = { version = "2.0.0-rc.10", optional = true, features = ["download-binaries", "cuda", "tensorrt"] }
ndarray = { version = "0.16", optional = true }
tokenizers = { version = "0.15.0", optional = true }
hf-hub = { version = "0.3.2", optional = true, default-features = false, features = ["tokio", "online"] }

[dev-dependencies]
tokio = { workspace = true, features = ["macros", "rt", "rt-multi-thread", "time"] }
criterion = { version = "0.5", features = ["async_tokio"] }

[features]
default = ["python-bridge"]  # Python bridge with ONNX+CoreML (most reliable)
mlx = ["pyo3"]               # MLX embedding provider (fallback, requires Python/PyO3)
onnx = [                     # Pure Rust ONNX Runtime (CPU-only unless custom build)
    "ort",
    "ndarray",
    "tokenizers",
    "hf-hub"
]
python-bridge = []           # Python subprocess bridge with ONNX+CoreML EP (recommended)
